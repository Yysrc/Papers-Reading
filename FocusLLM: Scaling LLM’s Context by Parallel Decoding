# 读论文：FocusLLM: Scaling LLM’s Context by Parallel Decoding

论文背景：

1. **LLM 长上下文建模的需求**：许多应用场景（复杂文档分析、生成连贯长篇文本、问答任务）涉及到长上下文的建模，更长的上下文有助于 LLM 更全面地理解输入内容，生成更高质量的输出

2. **利用长上下文的挑战**：计算复杂度高（二次方级别）；外推性能差（即使经过额外的微调）；高质量的长文本数据集难以获取
3. **现有方法的不足**：许多现有方法通过修改注意力机制或压缩 token 来实现理论上的无限长度，但这些方法往往会导致模型在长文本中对早期部分信息的丢失，从而影响其性能

*$\blacktriangle$ 思考：*

*想要建模超出模型默认上下文长度的文本，一个简单的想法就是将提示词拆分为多个片段（每个片段包含若干个 tokens），对每个片段使用一个单独的模型，进行并行的前向传播。这些模型负责聚合片段的内容，每个模型生成一个 token，这些 token 拼接后输入另一个解码器模型，得到最终的输出。这是最初步的想法，还存在一些问题，例如，这些处理片段的模型只能关注到局部的上下文等。论文的实现解决了这些问题，具体的实现细节如下*

### 具体方法

给定包含 $S$ 个 tokens 的长序列 $\{x_1, ..., x_S\} $，将其分割为两部分：

- **memory tokens**：$\{x_1, ..., x_m\}$，即前半部分
- **local tokens**：$\{x_{m+1}, ..., x_S\}$，即后续部分

其中 local tokens 长度不超过模型的默认上下文长度 $L$。同时将 memory tokens 分割为若干块 $C_1, C_2, ..., C_k$，每块包含若干个 tokens，大小也不超过 $L$。在每个块的后面附加一小段 local tokens，对每块使用一个单独的模型，进行并行的解码。论文认为，这种方法（附加 local tokens）可以有效地保留全局信息。这一过程的符号定义如下：
$$
\hat{C_i} \leftarrow \{C_i; x_{m+j}, ..., x_S\}, \quad i = 1, ..., k;\quad 1 \leq j \leq S-m
$$
其中 $j$ 是超参数，决定了附加到每个块的 local tokens 数量。为减少计算开销，无需附加所有的 local tokens。在实验中采用默认长度 512 个 tokens 进行推理，这足以包含必要的局部上下文信息

*$\blacktriangle$ 思考：*

*为解决不同模型只能关注局部上下文的问题，论文给出的方法是，在每个块之后附加一定数量的 local tokens，也就是序列的最后部分。这里我的理解是，因为序列最后部分的 tokens 一般是最重要的，这部分通常会明确指出模型需要完成的任务，包含了序列的主题。所以将这一部分附加到每个块上，会让每一个解码模型都理解序列的主题是什么。但是我个人认为，该方法无法让模型关注到其他块内的上下文。或许可以同样从其他块内取一些 tokens 附加到当前块上，这样可以让模型关注到范围更大的上下文*

接下来是并行解码部分。**解码**的含义是，根据当前的文本生成下一个 token。在这一部分，每个解码 LLM 根据对应的块，并行地生成 **candidate tokens**，将这些 candidate tokens 聚合，得到 **final tokens**，也就是最终的解码器模型的输入。因为 candidate tokens 并不作为最后的输出，论文将这一过程看作**隐式**的解码。candidate tokens 的作用是表示当前块 $\hat{C_i}$ 的信息。这一过程是对当前块信息的整合与压缩

下面解释 candidate tokens 的生成方法。为了让模型更好地适应“隐式解码”的任务，同时尽可能保持原始模型的泛化能力，论文对模型进行了小幅度的修改，在模型每一层的权重矩阵上添加了一组新的可训练参数：
$$
\{W'_Q, W'_K, W'_V, W'_O\}_l
$$
其中，$W'_Q, W'_K, W'_V, W'_O$ 分别表示查询、键、值和输出矩阵的新参数，$l$ 表示层数。修改后的解码器模型记作 $F'_{dec}$。这一模型在每个块上并行解码，并从每个块中获得一个 candidate token，表示为 $e_i$：

$$
e_i = F'_{dec}(\hat{C_i})
$$
candidate tokens 在自注意力模块中的输出计算方式如下（$H \in R^{d_{dec}}$ 是 candidate tokens 的输入隐藏状态，$\oplus$ 表示矩阵的拼接，$K, V$ 是一个块中正常 token 的表示）：

$$
Q_e \leftarrow HW'_Q, \quad K_e \leftarrow HW'_K, \quad V_e \leftarrow HW'_V
$$

$$
A \leftarrow \text{softmax} \left( Q_e (K \oplus K_e)^T \right)
$$

$$
O_e \leftarrow V_e{W'}_O^T   \quad V_e \leftarrow A(V \oplus V_e^T)
$$

上述计算方法来自论文[Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://arxiv.org/abs/2401.03462)。最后生成的 candidate token 与 local token 进行拼接，并随后通过一个冻结的解码器模型得到输出。注意从每个块获取 candidate token 的过程是并行的前向传播，因此将这一过程称为**并行解码**

修改后的模型的训练方法是简单的自回归方法。因为模型生成的 candidate token 要从每个块中聚合有用信息，因此，损失仅应用于 local token。损失函数如下：
$$
\min_{F'_{dec}} - \sum_{i=2}^{L} \log(p(x_i \mid e_1, \ldots, e_k, x_1, \ldots, x_{i-1}; F'_{dec}))
$$
*$\blacktriangle$ 思考：*

*根据这个损失函数的形式，对 $\hat{C_i}$ 解码的模型和生成最终输出的模型是相同的。这里我不太理解，为什么针对不同的任务，选择了相同的模型呢？*

### 效率分析

Transformer 架构中的主要计算负担在于注意力机制，其复杂度为 $O(L^2)$，其中 $L$ 表示总序列长度。通过将序列分割为 $n$ 个块，每个块内复杂度变为 $O((L/n)^2)$。并行处理这些块时，时间复杂度也是 $O((L/n)^2)$



















